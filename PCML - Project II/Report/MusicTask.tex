\subsection{Music recommendation}
Collaborative filtering is the part of recommender systems that predicts users' preferences for particular items. The major challenge in predicting users'listening count, as in our task, is that the available user-artist counts are usually too few and sometimes a method's performance relies on a good initial estimation of the unknown entries. 
These are reasons why we decided to implement besides the common Kmeans, KNN also ALS[cite here], which uses only the known listening counts and avoids dependency on initial estimations.

\subsection{Data description}
The training data consists in a matrix $Ytrain$ of size $1774x15082$, corresponding to $1774$ users and $15802$ artists. Element $Ytrain(i,j)$ expresses how many times user i has listened to artist j. An entry of 0 means we do not have information for that (user, artist, count) triple.
We are also given the friendship graph of the $1774$ users stored as an adjacency matrix.

\subsection{Exploratory Data Analysis}

The listening counts matrix is very sparse with a density of only $0.0026\%$, corresponding to 
$69617$ (user, artist, count) triples. 1262 artists had 0 listening counts.
The variance of the entries is very high, the highest count is $352698$ while the average listening count per user and per artist are 5.52 and 5.46 respectively.

A histogram of all the listening counts, shown in Fig\ref{fig:count_distribution} tells us that the lknown entries follow a heavy tail distribution. The long tail contains a small number of popular items, the well-known artists, and the rest are located in the heavy tail.
One method to transform skewed data such that it becomes more gaussian distributed is to use the Box-Cox transform[cite]\\
\begin{table}[h]
  \centering
  \begin{tabular}{c  c }
  $data(\lambda)$&= $log(data), \lambda = 0$ \\ 
                            &= $\frac{data^\lambda - 1}{\lambda} ,\lambda \neq 0$ \\ 
  \end{tabular}
\end{table}
. In our case, we can choose $\lambda=0$ because our values are very high and positive. This transformation will make the distances between listening counts much smaller and will reduce the influence on RMSE of the (user,artist,count) triples in the long tail. 

\begin{figure}[h]
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
   \includegraphics[width=\textwidth]{figures/histYtrain_crop.pdf}
    \caption{Before data transformation}
  \end{subfigure}
  \begin{subfigure}[b]{0.45\textwidth}
    \includegraphics[width=\textwidth]{figures/histLogYtrain_crop.pdf}
    \caption{After data transformation}
  \end{subfigure}
  \caption{Distribution of all listening counts}
  \label{fig:count_distribution}
\end{figure}

\begin{figure}[h]
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
   \includegraphics[width=\textwidth]{figures/histCountPerUser.pdf}
    \caption{}
  \end{subfigure}
  \begin{subfigure}[b]{0.45\textwidth}
    \includegraphics[width=\textwidth]{figures/histCountPerArtist.pdf}
    \caption{}
  \end{subfigure}
  \caption{Distribution per user (a) and per artist(b) log of listenining count distribution}
  \label{fig:user_artist_distribution}
\end{figure}

 
\subsection{Task 1}
In all our experiments we used 10-fold cross validation.
For the first task, we randomly omit 10 entries for every artist.
Splitting the data was more difficult since we needed to make sure we do not
remove all the entries for an artist. If an artist has $NA$ entries with $NA < 10$, then we keep $NA-1$ entries for testing,
 to still have one element for training.
We do not review here the details of ALS algorithm, the reader can consult the paper[cite] beforehand, since this was not the goal of this report.
The methods logALS and logKmeans transformed the training entries using log, but computed the RMSE undoing the transformation, by taking exp of the result.
 
\subsection{Baseline}
There are three simple basic predictions one can try: the global average count, the average count per user and the average count
per artist prediction. All of these methods give similar RMSE results: 3356, 3293 and 3496 respectively.
We note that the final RMSE is composed of various values, small and large. In Fig we can see a ploto of the RMSE error terms in log format.

\subsubsection{KNN}
\subsubsection{ALS}

\subsubsection{logALS}
Using 20 features and $lambda in [0.01,0.05,0.1,0.5,1]$ we obtain the results in table \ref{table:labda_choice} using 10-fold cross validation .The experiments were repeated twice with different seed. We note that we stopped the update steps in the algorithm only after 5 iterations because the update step was very computational intensive.
\begin{center}
  \begin{tabular}{ |l | c | c| }
    \hline
     lambda & RMSE train & RMSE test \\ \hline
     0.01   & 5075 ($\pm$  1.0793e+10) & 1.13 ($\pm$ 0.05) \\ \hline
     0.05  &  3355 ($\pm$  895)            &                1.14 ($\pm$  0.02)                   \\ \hline
     0.1     & 3350 ($\pm$ 851)  & 1.58 ($\pm$ 0.06) \\ \hline
     0.5    & 3341  ( $\pm$ 851)   &13.93 ($\pm$ 0.86)\\ \hline
     1       & 3387 ($\pm$ 842)   &25.30($\pm$ 1.48)\\ \hline
     1.5    & 3416 ($\pm$ 837) & 44.25($\pm$ 2.93) \\
    \hline
  \end{tabular}
  	\label{table:labda_choice}
    \captionof{table}{Estimated Train and Test RMSE for the two blob models.}
\end{center}
We can see that a value of lambda in the middle is a reaasonable choise, so we selected lambda to be 0.1 for the next experiments.
Our algorithm did a good job training  with a RMSE less than  3 but a bad job on unseen data with RMSE > 3000, meaning it is overfitting 
the training data and incrasing the value of lambda, the regularization parameter did not help.


We notice that the test error across the 10 different splits of the cross validation has values from 2000 to up to 6000. This is an artifact of the high listening counts present in the long tail and the randomness of the split.
\begin{figure}[h]
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
   \includegraphics[width=\textwidth]{figures/distributionRMSE.pdf}
    \caption{}
  \end{subfigure}
  \begin{subfigure}[b]{0.45\textwidth}
    \includegraphics[width=\textwidth]{figures/histCountPerArtist.pdf}
    \caption{}
  \end{subfigure}
  \caption{Distribution per user (a) and per artist(b) log of listenining count distribution}
  \label{fig:new_plot}
\end{figure}

We varied the number of features from 10,20,30,40,50,100 with $\lambda = 0.1$ and repeteated the experiments with different seed.
 Giving train  between 1.24 increasing up to  2.4 for 19 features.  The test RMSE  
 was always in the interval [3569,3546]. None of this results results proved meaningful.

\begin{figure}[h]
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
   \includegraphics[width=\textwidth]{figures/als_train.pdf}
    \caption{}
  \end{subfigure}
  \begin{subfigure}[b]{0.45\textwidth}
    \includegraphics[width=\textwidth]{figures/als_test.pdf}
    \caption{}
  \end{subfigure}
  \caption{Train and Test RMSE for logALS with varying number of features}
  \label{fig:new_plot}
\end{figure}

\subsubsection{Kmeans}
We implemented logKmeans clustering over users by first tranforming the data using log.
The reason for implementing Kmeans with clustering of users 
instead of artists, although the number of artists is 10 times larger than the number of users is to use the clusters obtained here also in Task 2.

Inspired by the friendship graph information, we tried Kmeans with
varying values from [3,10,20,30,40,50,100]. We plot the mean train and test error using 10 fold cross validation in figure below. 
Using only 3 friends both the train and the test error was high, while for 100 clusters the difference between train and test error became larger, giving signs of overfitting.
The missing entries in the matrix were initialized with the user average, although that did not have a huge impact on our results. We then sorted the users according to their average score and initialized the clusters with samples of sorted users to have a more equilibrated assignment of users to clusters.

We took log of the nonzeros entries of Ytrainnew and before 
computing MAE we tranformed the data back by taking the exp
of the predicted values.
We noticed also the fact that sometimes the training error of Kmeans
(reported using exp of data) had very small fluctuations and it was not always decreasig as the algorithm convergence properties would expect. This is due to the fact that Kmeans miminizes squared error but our cost is a little different since we transform the data using exp.

\begin{figure}[h]
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
   \includegraphics[width=\textwidth]{figures/kmeans_train_test.pdf}
    \caption{}
  \end{subfigure}
  \begin{subfigure}[b]{0.45\textwidth}
    \includegraphics[width=\textwidth]{figures/kmeans_train_test.pdf}
    \caption{}
  \end{subfigure}
  \caption{bla bla}
  \label{fig:kmeans_final}
\end{figure}

In all the experiments with more than 20 clusters obtained a test MAE close to 0.64. 
For computational reasons, we picked 20 clusters for the next task.
We also tried KMeans where in the update rule of the clusters were using only the artists that were currently rated, but it have problems with overfitting, giving a train error less than 0.4 much less than the normal Kmeans but a test error a bit over 0.7, higher than the normal KMeans.

\subsection{Task 2}
\subsubsection{Friendship information}
The initial friendship graph Gtrain of 1776 nodes and 22904 edges contained 22 connected components, but the majority of the components contained just a few nodes.
Using the Gephi tool[cite] we were able to find some properties of the graph such as the number of communities, 32.  Out of these 32 communities, only 8 communities had more than 150 members.
These statistics were run so that we get an idea of the number of user clusters present if friendship information is used. The number of connected components and of communities are similar with our choice of LogKMeans of 20-30 clusters.

In this task we are given a set of new users and their friendship information with a set of users
for which some listening counts are available. The challenge is to predict the listening counts for the new users based on the information given. In other words, we try to see if there is a correlation between
users' friendship and users' preference in music.

We have tried two approaches using the friendship information, one based on the mean average per user and one based on kmeans clustering per user.

\subsubsection{Baseline Methods}
As a baseline method, we predict for a missing value in the new users set the global average of all the available counts, which gave us a MAE of 1.07 8($\pm$0,044). Taking the average per artist gave us a MAE of 1.767($\pm$  0.09), significantly worse than the above.

\subsection{Using Friendship information}
In the first approach, an entry of a new user $Ystrong(u,a)$ is computed as the average count of its friends'listening counts $(f,a)$ for that artist or global average count for an user without friends (sad).

\begin{table}[h]
  \centering
  \begin{tabular}{ c  l }
  $Ystrong(u,a) $&= $\frac{\sum_{f\in Friends(u), Ytrain(f,a)\neq0}{Ytrain(f,a)}}{n\_fa}, n\_fa \neq 0$ \\ 
                          &= $global\_average, n\_fa = 0$ \\ 
  \end{tabular}
\end{table}
where $n\_fa$ is the cardinality of the set $\left\{ Ytrain(f,a)\neq0, f\in Friends(u)\right\}$

For this setup  we obtain  1.52 and 0.2911 which is smaller than our global average baseline.
If we also take into account the friends of the friends of user u we obtain  a MAE of 1.08($\pm$ 0.041). This result is comparable with the global average result.

\subsubsection{KMeans}
We cluster the users into 20 groups using the KMeans setup from Task 1
and repeat the experiments. For a new user, we predict the rating as the weighted mean of its friends' cluster. This gave us a MAE of
1.08 , 0.04.
Another try where the mean was done over the unique clusters or using only the most common cluster, gave us MAE of 1.49.
Another try with 100 friends 1.55.

Although KMeans with 20 features behaves similar with the baseline global average, we decided to use this on our newly predicted data.

\subsection{Summary}
Skwes the results, it is more important to have correct predictions on the users with hight counts. Looking back, we could have kep track of the max and median value of the test error to asses the accuracy of our results.

Using Vincent D Blondel, Jean-Loup Guillaume, Renaud Lambiotte, Etienne Lefebvre, Fast unfolding of communities in large networks, in Journal of Statistical Mechanics: Theory and Experiment 2008 (10), P1000 and the Gephi tool to find properties of the graph, it suggested it has about
32 communities, of which 8 had more than 150 members and the others being very small. (22 connected components, of which 20 are very small < 50 users.) This was run to give us an idea of the number of clusters we could use in Kmeans.


